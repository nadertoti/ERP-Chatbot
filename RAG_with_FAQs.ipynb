{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlZ8TnSVP6jWM4cXrJUrP4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadertoti/ERP-Chatbot/blob/main/RAG_with_FAQs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet pymongo sentence_transformers einops langchain langchain_community pypdf huggingface_hub PyPDF2"
      ],
      "metadata": {
        "id": "xwCjEebifdkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pymongo\n",
        "from pymongo.operations import SearchIndexModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MongoDB connection setup\n",
        "MONGO_URI = \"mongodb+srv://nadertoti4:UU1h8bHKSG2msUhh@chatbot.3g0if.mongodb.net/?retryWrites=true&w=majority&appName=chatbot\"\n",
        "client = pymongo.MongoClient(MONGO_URI)\n",
        "db = client[\"test\"]\n",
        "source_collection = db[\"knowledgebases\"]  # Original data collection\n",
        "embedded_collection = db[\"embedded_data\"]  # Collection for embeddings\n",
        "\n",
        "# Load SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "NWuCjstxuwYT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate embeddings and store in embedded_data collection\n",
        "def generate_and_store_embeddings():\n",
        "    print(\"Generating embeddings for knowledgebase...\")\n",
        "    embedded_collection.delete_many({})  # Clear existing data\n",
        "    cursor = source_collection.find()\n",
        "\n",
        "    for doc in tqdm(cursor, desc=\"Processing documents\"):\n",
        "        question = doc.get(\"question\", \"\")\n",
        "        response = doc.get(\"response\", \"\")\n",
        "        combined_text = f\"Question: {question} Response: {response}\"\n",
        "\n",
        "        # Generate embeddings\n",
        "        embedding = embedding_model.encode(combined_text).tolist()\n",
        "\n",
        "        # Store in embedded_data collection\n",
        "        embedded_doc = {\n",
        "            \"question\": question,\n",
        "            \"response\": response,\n",
        "            \"embedding\": embedding\n",
        "        }\n",
        "        embedded_collection.insert_one(embedded_doc)\n",
        "    print(\"Embeddings generated and stored successfully!\")"
      ],
      "metadata": {
        "id": "n7qWOMK1zIiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure vector search index exists\n",
        "def create_vector_index():\n",
        "    print(\"Creating vector index on embedded_data collection...\")\n",
        "    search_index_model = SearchIndexModel(\n",
        "        definition={\n",
        "            \"fields\": [\n",
        "                {\n",
        "                    \"type\": \"vector\",\n",
        "                    \"numDimensions\": 384,  # SentenceTransformer dimensions\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"similarity\": \"cosine\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        name=\"vector_index\",\n",
        "        type=\"vectorSearch\"\n",
        "    )\n",
        "    embedded_collection.create_search_index(model=search_index_model)\n",
        "    print(\"Vector index created successfully!\")"
      ],
      "metadata": {
        "id": "KVjZ2EcszQev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents using vector search\n",
        "def get_query_results(query):\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"exact\": True,\n",
        "                \"limit\": 5\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"question\": 1,\n",
        "                \"response\": 1,\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    results = embedded_collection.aggregate(pipeline)\n",
        "    return list(results)\n",
        "\n",
        "# Load Hugging Face LLM\n",
        "def load_llm():\n",
        "    os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "    return InferenceClient(\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        token=os.getenv(\"HF_TOKEN\")\n",
        "    )\n",
        "\n",
        "# RAG pipeline for chat\n",
        "def chat_with_knowledgebase(query, llm):\n",
        "    context_docs = get_query_results(query)\n",
        "    context_string = \"\\n\".join(\n",
        "        [f\"Q: {doc['question']} A: {doc['response']}\" for doc in context_docs]\n",
        "    )\n",
        "    prompt = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        {context_string}\n",
        "        Question: {query}\n",
        "    \"\"\"\n",
        "    output = llm.chat_completion(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return output.choices[0].message.content\n",
        "\n",
        "# Main application loop\n",
        "def main():\n",
        "    # Load LLM and generate embeddings\n",
        "    llm = load_llm()\n",
        "    # generate_and_store_embeddings()\n",
        "    # create_vector_index()\n",
        "\n",
        "    print(\"Welcome to the Knowledgebase Chat Application!\")\n",
        "    while True:\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Ask a question\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter your choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                response = chat_with_knowledgebase(query, llm)\n",
        "                print(f\"\\nResponse:\\n{response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"Exiting the application. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwdnnIQhzVAH",
        "outputId": "1e044dfa-a970-4078-e28d-1277f2356e1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Knowledgebase Chat Application!\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Exit\n",
            "Enter your choice: 1\n",
            "Enter your question: supervisor name\n",
            "\n",
            "Response:\n",
            "The supervisor name is Dr.Emtinan Isameldin Omer.\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Exit\n",
            "Enter your choice: 1\n",
            "Enter your question: who is the model owner ?\n",
            "\n",
            "Response:\n",
            "The owner of the architectural model is Nader Eltayeb.\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Exit\n",
            "Enter your choice: 2\n",
            "Exiting the application. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}