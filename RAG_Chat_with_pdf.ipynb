{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML9gfifuTMvTdD20Fbihjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadertoti/ERP-Chatbot/blob/main/RAG_Chat_with_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet pymongo sentence_transformers einops langchain langchain_community pypdf huggingface_hub PyPDF2"
      ],
      "metadata": {
        "id": "kzdcmJzAs9zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the vector search index exists\n",
        "def create_vector_index():\n",
        "    search_index_model = SearchIndexModel(\n",
        "        definition={\n",
        "            \"fields\": [\n",
        "                {\n",
        "                    \"type\": \"vector\",\n",
        "                    \"numDimensions\": 384,  # Adjusted to match SentenceTransformer output\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"similarity\": \"cosine\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        name=\"vector_index\",\n",
        "        type=\"vectorSearch\"\n",
        "    )\n",
        "    collection.create_search_index(model=search_index_model)"
      ],
      "metadata": {
        "id": "LO8gjRxQz1bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "nQjOOj45z__2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings and save to MongoDB\n",
        "def save_to_mongodb(text, model):\n",
        "    sentences = text.split(\"\\n\")  # Split into chunks (modify for larger PDFs)\n",
        "    for sentence in tqdm(sentences, desc=\"Processing text\"):\n",
        "        if sentence.strip():  # Skip empty lines\n",
        "            embedding = model.encode(sentence).tolist()\n",
        "            collection.insert_one({\"text\": sentence, \"embedding\": embedding})"
      ],
      "metadata": {
        "id": "uUovCaWW0KGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pymongo\n",
        "from pymongo.operations import SearchIndexModel\n",
        "from PyPDF2 import PdfReader\n",
        "from huggingface_hub import InferenceClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MongoDB connection setup\n",
        "MONGO_URI = \"mongodb+srv://nadertoti4:UU1h8bHKSG2msUhh@chatbot.3g0if.mongodb.net/?retryWrites=true&w=majority&appName=chatbot\"\n",
        "client = pymongo.MongoClient(MONGO_URI)\n",
        "db = client[\"rag_db\"]\n",
        "collection = db[\"test\"]\n",
        "\n",
        "# Retrieve relevant documents using vector search\n",
        "def get_query_results(query, model):\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"exact\": True,\n",
        "                \"limit\": 5\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"text\": 1,\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)\n",
        "\n",
        "# Load Hugging Face LLM\n",
        "def load_llm():\n",
        "    os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "    return InferenceClient(\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        token=os.getenv(\"HF_TOKEN\")\n",
        "    )\n",
        "\n",
        "# RAG pipeline for chat\n",
        "def chat_with_pdf(query, model, llm):\n",
        "    context_docs = get_query_results(query, model)\n",
        "    context_string = \" \".join([doc[\"text\"] for doc in context_docs])\n",
        "    prompt = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        {context_string}\n",
        "        Question: {query}\n",
        "    \"\"\"\n",
        "    output = llm.chat_completion(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return output.choices[0].message.content\n",
        "\n",
        "# Main application loop\n",
        "def main():\n",
        "    # Load embedding model and LLM\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    llm = load_llm()\n",
        "\n",
        "    # Create vector index\n",
        "    # create_vector_index()\n",
        "\n",
        "    print(\"Welcome to the PDF Chat Application!\")\n",
        "    while True:\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Upload a PDF\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "        choice = input(\"Enter your choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            '''\n",
        "            file_path = input(\"Enter the path to your PDF file: \")\n",
        "            try:\n",
        "                text = extract_text_from_pdf(file_path)\n",
        "                save_to_mongodb(text, embedding_model)\n",
        "                print(\"PDF content uploaded and processed successfully!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing PDF: {e}\")\n",
        "            '''\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                response = chat_with_pdf(query, embedding_model, llm)\n",
        "                print(f\"\\nResponse:\\n{response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Exiting the application. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_3LRFZA3csX",
        "outputId": "50674b04-0cba-4d1b-b0f6-9dd6170341d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the PDF Chat Application!\n",
            "\n",
            "Options:\n",
            "1. Upload a PDF\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice: 1\n",
            "\n",
            "Options:\n",
            "1. Upload a PDF\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice: 3\n",
            "Exiting the application. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}