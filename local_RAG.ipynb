{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgGYZiTADQwR7p+SoaxGNp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadertoti/ERP-Chatbot/blob/main/local_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHW3C7ruqZ8p"
      },
      "outputs": [],
      "source": [
        "pip install --quiet pymongo gpt4all sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Replace with your MongoDB Atlas connection string\n",
        "ATLAS_CONNECTION_STRING = \"mongodb+srv://nadertoti4:UU1h8bHKSG2msUhh@chatbot.3g0if.mongodb.net/?retryWrites=true&w=majority&appName=chatbot\"\n",
        "\n",
        "# Connect to your local MongoDB instance or Atlas Cluster\n",
        "client = MongoClient(ATLAS_CONNECTION_STRING)\n",
        "\n",
        "# Select the `test.knowledgebases` collection\n",
        "source_collection = client[\"test\"][\"knowledgebases\"]\n",
        "\n",
        "# Load the embedding model\n",
        "model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')\n",
        "\n",
        "# Define a function to generate embeddings\n",
        "def get_embedding(text):\n",
        "    return model.encode(text).tolist()\n",
        "\n",
        "# Create the `test.embedded_data` collection in the same database\n",
        "embedded_collection = client[\"test\"][\"embedded_data\"]\n",
        "\n",
        "# Process all documents in the source collection\n",
        "for document in source_collection.find():\n",
        "    # Extract the `question` and `response` fields\n",
        "    question = document.get('question')\n",
        "    response = document.get('response')\n",
        "\n",
        "    # Generate embeddings if both fields exist\n",
        "    if question and response:\n",
        "        question_embedding = get_embedding(question)\n",
        "        response_embedding = get_embedding(response)\n",
        "\n",
        "        # Create a new document with the embeddings\n",
        "        embedded_doc = {\n",
        "            \"_id\": document[\"_id\"],  # Maintain original document ID\n",
        "            \"question\": question,    # Store the original question\n",
        "            \"response\": response,    # Store the original response\n",
        "            \"question_embedding\": question_embedding,  # Embedding for the question\n",
        "            \"response_embedding\": response_embedding   # Embedding for the response\n",
        "        }\n",
        "\n",
        "        # Insert the new document into the `embedded_data` collection\n",
        "        embedded_collection.insert_one(embedded_doc)\n",
        "\n",
        "print(\"Embeddings generated and stored in test.embedded_data collection.\")\n"
      ],
      "metadata": {
        "id": "6O0HS9xXXJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.operations import SearchIndexModel\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB Atlas\n",
        "client = MongoClient(\"mongodb+srv://nadertoti4:UU1h8bHKSG2msUhh@chatbot.3g0if.mongodb.net/?retryWrites=true&w=majority&appName=chatbot\")\n",
        "db = client[\"test\"]\n",
        "collection = db[\"embedded_data\"]\n",
        "\n",
        "# Define the search index model\n",
        "search_index_model = SearchIndexModel(\n",
        "    definition={\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"numDimensions\": 1024,  # Matches your embedding dimensions\n",
        "                \"path\": \"question_embedding\",  # Index the embeddings field\n",
        "                \"similarity\": \"cosine\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    name=\"vector_index\",\n",
        "    type=\"vectorSearch\"\n",
        ")\n",
        "\n",
        "# Create the vector search index\n",
        "collection.create_search_index(model=search_index_model)\n",
        "print(\"Vector Search Index Created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaR_QBe5fyZn",
        "outputId": "827072cf-9dda-42d5-d611-ed57efc441cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Search Index Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Ensure numpy is installed\n",
        "from pymongo import MongoClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Connect to MongoDB Atlas\n",
        "client = MongoClient(\"mongodb+srv://nadertoti4:UU1h8bHKSG2msUhh@chatbot.3g0if.mongodb.net/?retryWrites=true&w=majority&appName=chatbot\")\n",
        "db = client[\"test\"]\n",
        "collection = db[\"embedded_data\"]\n",
        "\n",
        "# Function to get embeddings (you need to implement or use a pre-trained model)\n",
        "def get_embedding(text):\n",
        "    # Replace this with your embedding generation logic\n",
        "    # Example: return np.random.rand(1024).tolist() for testing\n",
        "    return model.encode(text).tolist()\n",
        "    #return np.random.rand(1024).tolist()\n",
        "\n",
        "# Function to perform vector search\n",
        "def get_query_results(query):\n",
        "    query_embedding = get_embedding(query)\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"question_embedding\",\n",
        "                \"exact\": True,\n",
        "                \"limit\": 5\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"question\": 1,\n",
        "                \"response\": 1,\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)\n",
        "\n",
        "# Test the function\n",
        "query = \"Who worked on the project?\"\n",
        "results = get_query_results(query)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKO2yuVxhfKL",
        "outputId": "48a58d90-8f16-4e80-d38e-0a6bddf4e0fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'question': 'who work on the project ? ', 'response': 'Nader and Farah', 'score': 0.9574169516563416}, {'question': 'who is the owner of the architectural model ? ', 'response': 'Nader Eltayeb', 'score': 0.7542623281478882}, {'question': 'What is your supervisor name?', 'response': 'Dr.Emtinan Isameldin Omer', 'score': 0.7283762693405151}, {'question': 'What is company name?', 'response': 'MTN Sudan telecom company', 'score': 0.7259685397148132}, {'question': 'How can I contact with them', 'response': 'Just call 123', 'score': 0.7129998207092285}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpt4all\n"
      ],
      "metadata": {
        "id": "O4CgiFnAkT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "import os\n",
        "\n",
        "# Download the model if it doesn't exist\n",
        "model_name = \"mistral-7b-openorca.gguf2.Q4_0.gguf\"  # or the correct model name\n",
        "local_llm_path = f\"./models/{model_name}\"\n",
        "\n",
        "# Check if the model exists\n",
        "if not os.path.exists(local_llm_path):\n",
        "    # Create the 'models' directory if it doesn't exist\n",
        "    os.makedirs(\"./models\", exist_ok=True)  # This line is added\n",
        "\n",
        "    print(f\"Model not found at {local_llm_path}. Downloading...\")\n",
        "    GPT4All.download_model(model_name, model_path=\"./models\")  # Download to the models directory\n",
        "    print(\"Model downloaded.\")\n",
        "else:\n",
        "    print(f\"Model found at {local_llm_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AuBQu7PkYkm",
        "outputId": "b24f50e6-af34-40a5-fa00-1f9f88749170"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model found at ./models/mistral-7b-openorca.gguf2.Q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "# Load the local LLM\n",
        "local_llm_path = \"/content/models/mistral-7b-openorca.gguf2.Q4_0.gguf\"\n",
        "local_llm = GPT4All(local_llm_path)\n"
      ],
      "metadata": {
        "id": "wmt4nEWEo4QU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    # Retrieve relevant documents\n",
        "    documents = get_query_results(question)\n",
        "\n",
        "    # Prepare the context for the LLM\n",
        "    context = \"\"\n",
        "    for doc in documents:\n",
        "        question_text = doc.get(\"question\", \"\")\n",
        "        response_text = doc.get(\"response\", \"\")\n",
        "        context += f\"Q: {question_text}\\nA: {response_text}\\n\"\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"Use the following pieces of context to answer the question:\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the response\n",
        "    response = local_llm.generate(prompt)\n",
        "    cleaned_response = response.replace('\\\\n', '\\n')\n",
        "    return cleaned_response\n",
        "\n",
        "# Test the RAG pipeline\n",
        "question = \"model owner ?\"\n",
        "response = answer_question(question)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmVbgAPyncGW",
        "outputId": "b240c9d2-f9e1-498a-d76f-0ebc54afd4e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Answer: Nader Eltayeb\n"
          ]
        }
      ]
    }
  ]
}